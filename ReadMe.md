# Yul Yenâ€™s AI Orchestra

**Note (English):**  
This is a private project and repository. The short introduction is written in English in case someone outside Germany stumbles upon it â€“ but from here on the documentation continues in German, since the AI personas (Leah, Doris, Peter) are primarily designed to operate in German.  

*Yul Yenâ€™s AI Orchestra is a local AI project with multiple personas, designed for private use and experimentation.*

**Yul Yenâ€™s AI Orchestra** ist eine lokal laufende KI-Umgebung, die mehrere **Personas** (Leah, Doris, Peter) vereint.  
Sie alle basieren auf einem lokalen LLM (Ã¼ber [Ollama](https://ollama.com/) oder kompatible Backends) und bringen eigene Charaktere und Sprachstile mit.  

```mermaid
flowchart TD
  A[Dirigent: Julian / Yul Yen]

  subgraph Stimmen
    L[Leah - empathisch]
    D[Doris - sarkastisch]
    P[Peter - nerdig]
  end

  subgraph Core
    O[Ollama Runtime]
    W[WebUI Gradio]
    T[Terminal UI]
    M[LLM-Modelle: Leo13B, GPT-OSS-20B]
  end

  subgraph Wissen_und_Struktur
    K[Wiki-Proxy / Kiwix]
    CFG[Config yaml/json]
    LOG[Logging]
  end

  subgraph Zukunft
    R[RAG / Kontextkompression Karl]
    S[Tool-Use / TTS-STT]
  end

  A --> Stimmen
  A --> Core
  A --> Wissen_und_Struktur
  A --> Zukunft

  O --> M
  W -->|Nutzerinteraktion| X[Publikum]
  T -->|Nutzerinteraktion| X

  K -.->|Kontext-Snippets| L
  CFG -.->|Einstellungen| W
  CFG -.-> T
  LOG -.->|Analyse| A

  R -.-> L
  S -.-> W
```

Das Projekt unterstÃ¼tzt:
- **Terminal-UI** mit farbiger Konsolenausgabe & Streaming  
- **Web-UI** auf Basis von [Gradio](https://gradio.app) (im lokalen Netzwerk erreichbar)  
- **API (FastAPI)** zur Integration in externe Anwendungen  
- **Wikipedia-Integration** (online oder offline via Kiwix-Proxy)  
- **Logging & Tests** fÃ¼r stabile Nutzung  

---

## Ziele

- Bereitstellung einer **privaten, lokal laufenden KI** fÃ¼r deutschsprachige Interaktion  
- Mehrere **Charaktere mit unterschiedlichem Stil**:  
  - **Leah**: empathisch, freundlich  
  - **Doris**: sarkastisch, humorvoll  
  - **Peter**: faktenorientiert, analytisch  
- **Erweiterbares Fundament** fÃ¼r zukÃ¼nftige Features (z. B. LoRA-Finetuning, Tool-Use, RAG)  
- **KISS-Prinzip**: einfache, nachvollziehbare Architektur  

---

## ArchitekturÃ¼berblick

- **Konfiguration**: Alle Einstellungen zentral in `config.yaml`  
- **Core**:  
  - `OllamaStreamer` fÃ¼r LLM-Aufrufe & Streaming  
  - Wikipedia-Support inkl. spaCy-basiertem Keyword-Extractor  
- **Personas**: Systemprompts & Eigenheiten in `src/config/personas.py`  
- **UI**:  
  - `TerminalUI` fÃ¼r CLI  
  - `WebUI` (Gradio) mit Persona-Auswahl & Avataren  
- **API**: FastAPI-Server (`/ask`-Endpoint fÃ¼r One-Shot-Fragen)  
- **Logging**:  
  - ChatverlÃ¤ufe und Systemlogs in `logs/`  
  - Wiki-Proxy schreibt separate Logdateien  

---

## Voraussetzungen

- **Python 3.10+**  
- **Ollama** (oder anderes kompatibles Backend) mit installiertem Modell, z. B.:  
  ```bash
  ollama pull leo-hessianai-13b-chat:Q5
  ```  
- Optional fÃ¼r Offline-Wiki:  
  - [Kiwix](https://kiwix.org/) + deutsches ZIM-Archiv  

---


## Installation

```bash
git clone https://github.com/YulYen/YulYens_AI.git
cd YulYens_AI

# Virtuelle Umgebung
python -m venv .venv
source .venv/bin/activate   # Linux/macOS
.venv\Scripts\activate      # Windows

# AbhÃ¤ngigkeiten installieren
pip install -r requirements.txt
```

### Sprachmodell fÃ¼r spaCy

FÃ¼r die Wikipedia-Integration wird ein deutsches Sprachmodell benÃ¶tigt.  
Die Auswahl soll zukÃ¼nftig Ã¼ber die Konfiguration (`config.yaml`) erfolgen.  
ZusÃ¤tzlich muss das jeweilige Modell manuell installiert werden:

```bash
# Mittleres Modell (Kompromiss)
python -m spacy download de_core_news_md

# GroÃŸes Modell (genauer, aber etwas langsamer und grÃ¶ÃŸer)
python -m spacy download de_core_news_lg
```


---

## Nutzung

### Konfiguration (`config.yaml`)

Alle zentralen Einstellungen werden Ã¼ber `config.yaml` gesteuert. Beispiel:

```yaml
core:
  # Standardmodell fÃ¼r Ollama
  model_name: "leo-hessianai-13b-chat.Q5"
  # URL des lokal laufenden Ollamaâ€‘Servers (Protokoll + Host + Port).
  # Dieser Wert muss explizit gesetzt werden â€“ es gibt keinen stillen Default.
  ollama_url: "http://127.0.0.1:11434"
  # Warmâ€‘up: ob beim Start ein Dummyâ€‘Aufruf zum Modell geschickt wird.
  warm_up: false

ui:
  type: "terminal"   # Alternativen: "web" oder null (nur API)
  web:
    host: "0.0.0.0"
    port: 7860

wiki:
  mode: "offline"    # "offline", "online" oder false
  proxy_port: 8042
  snippet_limit: 1600
```

### Start

```bash
python src/launch.py
```

- **Terminal-UI**  
  - Eingabe: Fragen tippen  
  - Befehle: `exit` (beenden), `clear` (neue Unterhaltung)  

- **Web-UI**  
  - Startet automatisch bei `ui.type: "web"`  
  - Im Browser Ã¶ffnen: `http://127.0.0.1:7860`  
  - Persona auswÃ¤hlen, chatten  

- **API (FastAPI)**  
  ```bash
  curl -X POST http://127.0.0.1:8013/ask \
       -H "Content-Type: application/json" \
       -d '{"question":"Wer hat die RelativitÃ¤tstheorie entwickelt?"}'
  ```

---

## Beispiel

**Frage (Leah):**  
> Wer ist Angela Merkel?

**Antwort (gestreamt):**  
> Angela Merkel ist eine deutsche Politikerin (CDU) und war von 2005 bis 2021 Bundeskanzlerin der Bundesrepublik Deutschland. â€¦

---

## Tests

Mit [pytest](https://docs.pytest.org/) ausfÃ¼hren:  
```bash
pytest tests/
```

---

## Status

ğŸš§ **Work in Progress** â€“ stabil nutzbar, aber aktiv in Entwicklung.  
Privates Projekt, **nicht fÃ¼r Produktivbetrieb gedacht**.
